# Презентация: как LLM понимают, размышляют и отвечают

> Аудитория: знакомы с LLM и их применением. Цель — простая, верхнеуровневая механика работы без перегруза терминами, с реальными бизнес-примерами.

---

## Слайд 1. Титул и хук
**Заголовок:** ИИ и «сознание»: как модели **понимают → размышляют → отвечают**

**Ключевые пункты:**
- Объясняем механику без формул; только нужные понятия
- Зачем разбирать устройство на высоком уровне
- План из трёх макрофаз: Кодирование → Размышление → Генерация

**Краткое содержание:**
Задаём рамку разговора и обещание простоты: разберём внутреннюю «фабрику ответов» из трёх цехов и покажем, где она похожа на человека, а где — нет.

**Примеры (тип):** чат‑поддержка в e‑commerce; ассистенты в офисных пакетах; поисковые ответы с объяснением.

**Сходство/отличие (кратко):** Похоже — удерживаем контекст и стиль; отличается — нет личного опыта и целей.

---

## Слайд 2. Зачем знать устройство (для тех, кто уже пользуется LLM)
**Заголовок:** «Мы уже используем LLM — зачем копаться внутри?»

**Ключевые пункты:**
- Лучше ставить задачи: формулировки, ограничения, формат
- Понимать риски: фантазии, двусмысленность, отсутствие фактов
- Повышать качество: когда давать весь контекст, когда — идти пошагово
- Управлять стилем и стоимостью: токены, длина, температура

**Краткое содержание:**
Знание устройства даёт управляемость: мы точнее получаем то, что хотим; снижаем ошибки и стоимость. Это не «хакерство», а грамотное использование инструмента.

**Примеры (тип):** снижение FCR в поддержке; ускорение подготовки отчётов; сокращение токенов → уменьшение затрат.

**Сходство/отличие (кратко):** Похоже — «объясни задачу нормально — получишь лучше»; отличается — модель не догадается «по умолчанию», если не задать правила явно.

---

## Слайд 3. Карта процесса (птицей): три макрофазы
**Заголовок:** Кодирование → Размышление → Генерация

**Ключевые пункты:**
- Вход (текст/мультимедиа) → внутренние представления
- Размышление: выделение важного, сбор связей
- Генерация: по шагам выбираем следующий фрагмент
- Надстройки: факты (поиск), инструменты, правила/безопасность

**Краткое содержание:**
Даем простую ментальную модель конвейера. В реальности границы размыты, но разделение полезно для понимания и управления ответом.

**Примеры (тип):** маршрутизация тикета: контекст → поиск по базе → формирование ответа.

**Сходство/отличие (кратко):** Похоже — «подумал → сказал»; отличается — мысль и речь у модели часто происходят в одном механизме прогнозирования.

---

## Слайд 4. Вход и кодирование (кратко)
**Заголовок:** Из запроса — в «внутренний язык» модели

**Ключевые пункты:**
- Запрос + инструкции + (опц.) факты/результаты поиска
- Преобразование в числа (внутренние координаты смысла)
- Учет порядка и структуры

**Краткое содержание:**
Контекст вы уже знаете. Суть: всё приводится к единому внутреннему представлению, где «похожие смыслы ближе».

**Примеры (тип):** шаблонный промпт в службе поддержки; «ответь на русском, кратко, ссылками».

**Сходство/отличие (кратко):** Похоже — общая «карта смыслов»; отличается — модель не «понимает» словами, а работает с числами.

---

## Слайд 5. Что такое токен (отдельно)
**Заголовок:** Токен — минимальный кусочек текста для модели

**Ключевые пункты:**
- Не всегда слово: часто часть слова или знак
- Одно и то же слово может разбиться по‑разному в зависимости от модели
- Токены — «буквы» внутренней машины

**Краткое содержание:**
Токен — та единица, из которой модель «складывает» понимание и ответ. Знать о токенах важно для качества и стоимости.

**Примеры (тип):** «алгоритм», «привет!!!», «GPT‑4o‑mini», email‑адрес, URL — как разбиваются.

**Сходство/отличие (кратко):** Похоже — как слоги и знаки; отличается — нет «естественных» слов, только сегменты.

---

## Слайд 6. Особенности токенизации: числа, пунктуация, разметка
**Заголовок:** Почему «2490» и «2 490» — это разные истории

**Ключевые пункты:**
- Числа: формат влияет на разбиение и точность (2490 vs 2,490 vs 2 490)
- Пунктуация и повторы: «!!!», эмодзи, кавычки — отдельные токены
- Разметка: Markdown/HTML добавляют служебные сегменты
- Выводы: единообразие формата ↑ точность, ↓ стоимость

**Краткое содержание:**
Аккуратное форматирование помогает модели: стабильные числа, чистая пунктуация, уместная разметка.

**Примеры (тип):** расчёт НДС в финтех‑боте; парсинг заказов из почты; генерация описаний товаров с Markdown.

**Сходство/отличие (кратко):** Похоже — людям тоже легче с аккуратным текстом; отличается — для модели «лишний символ» реально меняет вычисление.

---

## Слайд 7. Мультимодальность и мультиязычность
**Заголовок:** Картинки, звук и разные языки — один «внутренний язык» внутри

**Ключевые пункты:**
- Изображение/аудио проецируются в то же пространство смыслов
- Переключение языков — как диалекты одной карты
- Практика: подпись к фото на одном языке, вопрос — на другом

**Краткое содержание:**
LLM «сшивает» разные виды входа в общую смысловую ткань, что упрощает кросс‑языковые и визуальные задачи.

**Примеры (тип):** разбор фото чека (retail), инструкция к устройству по картинке (support), смешанные языки в чате с клиентом.

**Сходство/отличие (кратко):** Похоже — человек тоже «видит‑слышит‑читает»; отличается — у модели нет «зрения/слуха», только преобразованные признаки.

---

## Слайд 8. Переход: от токенов — к тому, что модель «делает»
**Заголовок:** Что дальше происходит с токенами

**Ключевые пункты:**
- Модель много раз «перечитывает» вход, выделяя важное
- На каждом шаге усиливает связи и контексты
- Готовит основу для выбора следующего фрагмента ответа

**Краткое содержание:**
Плавный мостик от восприятия к мышлению: токены — это сырьё; дальше начинается работа по расстановке акцентов и подготовке к ответу.

**Примеры (тип):** извлечение даты из длинного письма; поиск адреса доставки в свободном тексте заказа.

**Сходство/отличие (кратко):** Похоже — перечитываем и подчеркиваем важное; отличается — делает это параллельно и математически.

---

## Слайд 9. Вероятности и «шум» — это фича, а не баг
**Заголовок:** Почему модель иногда даёт разные ответы на один запрос

**Ключевые пункты:**
- Ответ строится вероятностно; «температура» и top‑p управляют разнообразием
- «Шум» помогает креативу и вариативности формулировок
- Для точности — снижаем вариативность; для идей — повышаем

**Краткое содержание:**
Случайность контролируема. Мы сами выбираем баланс между стабильностью и новизной, регулируя параметры генерации.

**Примеры (тип):** генерация названий продукта (маркетинг); несколько вариантов письма клиенту; A/B тексты баннеров.

**Сходство/отличие (кратко):** Похоже — человек варьирует речь; отличается — у модели это ручка параметров, а не настроение.

---

## Слайд 10. Цикличность: модель пишет токен за токеном
**Заголовок:** Автогенерация: как растёт цепочка ответа

**Ключевые пункты:**
- Пишем по одному токену, каждый раз учитывая уже написанное
- Цепочка может «увести» в сторону при неопределённости
- Управление: ограничение длины, явные планы/структуры

**Краткое содержание:**
Механика ответа пошаговая. Ранние формулировки влияют на всё последующее — поэтому важны чёткий формат и опорные факты.

**Примеры (тип):** потоковый ответ в чате поддержки; автодополнение письма; генерация описания товара.

**Сходство/отличие (кратко):** Похоже — «оговорился — пошёл не туда»; отличается — у модели это чистая зависимость от уже выбранных токенов.

---

## Слайд 11. Где «мышление», а где «генерация» — и почему мы их разделяем
**Заголовок:** Рабочее разделение, хотя механизм общий

**Ключевые пункты:**
- В реальности «мысль» и «речь» рождаются одним прогнозатором
- Полезное разделение: подготовка контекста (мышление) vs выбор слов (генерация)
- Практика: факты и планы помогают «мышлению»; параметры и формат — «генерации»

**Краткое содержание:**
Это модель объяснения. Она удобна для проектирования промптов и сценариев: где добавить факты, а где ужесточить форму вывода.

**Примеры (тип):** перед ответом — подмешать выдержки из базы (мышление); затем — JSON‑схема для вывода (генерация).

**Сходство/отличие (кратко):** Похоже — «сначала подумал, потом сказал»; отличается — границы в модели не физиологические, а функциональные.

---

## Слайд 12. Управление стилем и форматом — почему это работает
**Заголовок:** Механика тональности и формы

**Ключевые пункты:**
- Инструкции в контексте смещают вероятности слов и структуры
- Чёткий формат (список/таблица/JSON) задаёт «рельсы» вывода
- Тон («для новичка/эксперта») меняет выбор терминов и длину

**Краткое содержание:**
Модель «слышит» стиль как условие задачи. Правильно заданные правила меняют сами вероятности формулировок — и итог выглядит именно так, как мы попросили.

**Примеры (тип):** письмо клиенту вежливо и кратко; юридическое резюме в пунктах; технический чек‑лист.

**Сходство/отличие (кратко):** Похоже — человеку тоже задают ТЗ; отличается — у модели нет вкуса, только следование вероятностям под условием.

---

## Слайд 13. Сходство и различия с человеком — сводка
**Заголовок:** Что в модели «по‑человечески», а что — машинное

**Ключевые пункты:**
- Похоже: удержание контекста, стиль, перефразирование, обобщение
- Отличается: нет убеждений/намерений; без фактов «угадывает» по статистике
- Практический вывод: сила в комбинации правил + фактов + формата

**Краткое содержание:**
Закрепляем мысль: правдоподобная речь ≠ знание. Делайте модель опирающейся на источники и правила — получите надёжный инструмент.

**Примеры (тип):** ссылка на базу знаний в ответах; контроль формата отчётов; запрет на домыслы.

**Сходство/отличие (кратко):** Похоже — звучит «как человек»; отличается — не «знает», а предсказывает.

---

## Слайд 14. Как это работает в разных кейсах (через механику модели)
**Заголовок:** Последовательно или «всё сразу»? Где узкие места

**Ключевые пункты:**
- **Последовательный подход** (step‑by‑step): лучше для сложных задач с условиями
- **Весь контекст в промпте**: удобно для простых FAQ и поиска ответа
- Алгебра и точные вычисления: ограничены; помогает калькулятор/пост‑проверка
- RAG (подмешивание фактов) снижает «фантазии» и повышает проверяемость

**Краткое содержание:**
Сопоставляем стратегии с устройством модели: где важнее пошаговое выделение важного, а где достаточно полного контекста. Показываем, как компенсировать слабые места.

**Примеры (тип):**
- Комплаенс‑проверка договора: RAG + пошаговое извлечение условий
- Поддержка FAQ: полный контекст в промпте + строгий формат ответа
- Расчёты в финтех: вынести математику во внешний инструмент

**Сходство/отличие (кратко):** Похоже — человек тоже делит сложное на шаги; отличается — модель не «считает», а выбирает вероятные токены.

---

## Слайд 15. Итоги
**Заголовок:** Коротко о главном

**Ключевые пункты:**
- Простая карта: **Кодирование → Размышление → Генерация**
- Качество = понятная постановка + факты + формат + контроль вариативности
- Модель — мощный, но статистический инструмент

**Краткое содержание:**
Закрываем круг: понимаем, из чего складывается ответ и как на него влиять. Отсюда — осознанное использование LLM в продуктах и процессах.

**Примеры (тип):** закрепляющие кейсы из вашей отрасли: краткий резюме‑слайд с 2–3 видами задач.

**Сходство/отличие (кратко):** Похоже — «понимает, думает, говорит»; отличается — предсказывает, а не знает.

---

### Дополнительные улучшения и ревью
- **Логика переходов:** добавлен явный мост (Слайд 8) от токенов к работе модели.
- **Токены:** вынесены в отдельный слайд; особые случаи (числа, пунктуация, разметка) — на своём слайде с практическими выводами.
- **Вероятности/«шум»:** объяснены как управляемая «фича» (Слайд 9), с бизнес‑контекстом (креатив, A/B).
- **Цикличность генерации:** выделена (Слайд 10), чтобы показать пошаговую природу ответа.
- **Разделение мышления/генерации:** пояснено как полезная модель, хоть механизм общий (Слайд 11).
- **Механика тональности:** объяснено, почему инструкции реально меняют ответ (Слайд 12).
- **Сходство/различия:** краткие ремарки на каждом слайде, итоговая сводка — на Слайде 13.
- **Кейсы по механике:** Слайд 14 формулирует практику «что когда работает лучше» через призму устройства модели.
- **Реальные примеры:** у каждого слайда указаны типы бизнес‑примеров для последующего наполнения.

