# Content-Reviewer Instructions: HITL/HOTL/HFTL Integration Assessment

## Review Context

**File to review:** `papers/blog1/post2_delegation/DRAFT_v3_HITL_FOCUS_RU.md`

**Review type:** Quality assessment + roast mode (maximum brutality)

**User request:** Rewrite blog with balanced approach - Hook + AI-human parallels → HITL/HOTL/HFTL methods → Decision framework for choosing method

**Key requirements:**
1. Reuse foundation from previous draft (lines 1-66)
2. Expand three methods with where works/doesn't work
3. Add actionable decision framework
4. Natural Russian language (blog tone)
5. 2,000-2,500 words target
6. Maintain adaptation/confidence framing

---

## Critical Review Dimensions

### 1. Structure & Reuse Assessment

**Check:**
- [ ] Foundation section (lines 1-80) properly reused from DRAFT_v2_ADAPTED_RU_REVISED.md
- [ ] Three methods (HITL/HOTL/HFTL) clearly separated and balanced
- [ ] Decision framework is standalone section (not buried in methods)
- [ ] Conclusion updated to emphasize method choice
- [ ] Total length within 2,000-2,500 words

**Roast points:**
- Is foundation truly reused or unnecessarily rewritten?
- Are method sections balanced in length and depth?
- Does decision framework stand out or blend into wall of text?

---

### 2. HITL/HOTL/HFTL Method Quality

**For EACH method, check:**
- [ ] Clear "what it is" definition
- [ ] Concrete "where works" examples with DATA
- [ ] Concrete "where doesn't work" examples with DATA
- [ ] Mechanisms explained (WHY it works/fails)
- [ ] Key insight summarizes learnings
- [ ] All claims cited with URLs

**HITL specific check:**
- [ ] Germany PRAIM study cited correctly (463,094 women, 17.6% improvement)
- [ ] Automation bias explained with data (79.7% → 19.8%)
- [ ] Complementarity paradox (g = -0.23, GPT-4 90% → 76%)
- [ ] Not just "HITL problems" but also "HITL successes"

**HOTL specific check:**
- [ ] Intesa Sanpaolo example concrete
- [ ] Stacks code review example (10-15% AI-generated)
- [ ] Clear differentiation from HITL (when to use each)
- [ ] "Where doesn't work" genuinely explained

**HFTL specific check:**
- [ ] Stream 80%+ automation cited
- [ ] McDonald's failure explained (80% vs 95%, shut down July 2024)
- [ ] Air Canada lawsuit ($1,630 tickets, lost lawsuit)
- [ ] Legal AI hallucinations (75% rate, $67.4B losses)
- [ ] Clear warning: only for predictable + low error cost

**Roast brutally:**
- Are examples generic or concrete?
- Are "where doesn't work" sections just token disclaimers or genuine analysis?
- Do mechanisms explain WHY or just state facts?
- Is balance between success/failure examples appropriate?

---

### 3. Decision Framework Actionability

**Check:**
- [ ] Four criteria clearly defined (cost of error, predictability, criticality, speed)
- [ ] Matrix or clear mapping of criteria to methods
- [ ] 3-step practical approach (start HITL → HOTL → HFTL)
- [ ] Signs of wrong method choice for each method
- [ ] Validation data (Ponemon 2.3x, $3.7M)
- [ ] Reader can apply immediately after reading

**Roast brutally:**
- Can reader ACTUALLY use this framework tomorrow? Or is it vague?
- Are criteria distinct or overlapping mush?
- Are "signs of wrong choice" actionable or obvious platitudes?
- Is matrix clear or confusing?
- Too simple (insults intelligence) or too complex (can't remember)?

---

### 4. Language & Tone Quality

**Check:**
- [ ] Natural Russian (not robotic translation)
- [ ] Blog tone (not academic paper)
- [ ] Conversational flow ("вот в чём парадокс", "представьте себе")
- [ ] No bullets in body paragraphs (narrative only)
- [ ] Technical terms in English where natural (HITL, automation bias)
- [ ] Dr. Elena Cognitive voice (warm, confident, evidence-based)

**Roast brutally:**
- Does it sound like native Russian blogger or Google Translate?
- Are transitions smooth or jarring?
- Is warmth genuine or forced?
- Are paragraphs readable or walls of text?
- Too casual (unprofessional) or too formal (boring)?

---

### 5. Evidence & Citation Quality

**Check:**
- [ ] Every statistic has inline URL citation
- [ ] Citations from tier 1 sources (Nature, JAMA, McKinsey, etc.)
- [ ] No invented statistics
- [ ] Numbers match source files (task2, task4, task5 summaries)
- [ ] Examples are real organizations (not hypothetical)

**Roast brutally:**
- Are citations working URLs or broken links?
- Are sources credible or random blogs?
- Are statistics accurate or inflated?
- Are examples verified or plausible fiction?

---

### 6. Adaptation Framing Maintained

**Check:**
- [ ] Message: "Adapt what you know" not "start from scratch"
- [ ] Confidence-building throughout (not panic-inducing)
- [ ] Classical management principles transferred (Drucker, Mintzberg)
- [ ] Methods presented as tools, not revolutionary new concepts
- [ ] Reader feels empowered, not overwhelmed

**Roast brutally:**
- Does framing drift to "everything is new and scary"?
- Is confidence genuine or hollow reassurance?
- Are methods presented as evolution or revolution?

---

### 7. Comparison with v2

**Compare DRAFT_v3 vs DRAFT_v2_ADAPTED_RU_REVISED.md:**

**What should be BETTER in v3:**
- [ ] Three methods clearly separated (not just HITL focus)
- [ ] Decision framework actionable (not vague recommendations)
- [ ] Where works/doesn't balance for each method
- [ ] Method selection guidance clear

**What should be PRESERVED from v2:**
- [ ] Foundation quality (Hook, Why now, What transfers)
- [ ] Natural Russian language
- [ ] Evidence quality
- [ ] Adaptation framing

**Roast brutally:**
- Is v3 genuinely better or just longer?
- Are additions valuable or filler?
- Was anything good from v2 lost in v3?
- Does v3 solve user's request or miss the point?

---

## Output Format

### Section 1: Overall Verdict

**Rating:** [APPROVE / NEEDS MINOR FIXES / NEEDS MAJOR REVISION / REJECT]

**One-sentence summary:** [Brutal honest assessment]

**Readiness percentage:** X% ready for publication

---

### Section 2: What Works Well (Strengths)

List 3-5 specific strengths with line number examples:
1. [Strength] - lines X-Y
2. [Strength] - lines X-Y

---

### Section 3: Critical Issues (Must Fix)

List issues blocking publication:
1. [Issue] - lines X-Y - [Specific fix needed]

---

### Section 4: Major Issues (Should Fix)

List issues significantly degrading quality:
1. [Issue] - lines X-Y - [Specific fix needed]

---

### Section 5: Minor Issues (Polish)

List polish-level improvements:
1. [Issue] - lines X-Y - [Quick fix]

---

### Section 6: Comparison with v2

**What's better:** [Specific improvements]
**What's worse:** [Specific regressions]
**What's missing:** [Good elements from v2 lost]

---

### Section 7: Roast (Maximum Brutality)

No holding back. Find every flaw:
- Weakest section?
- Most generic example?
- Least actionable advice?
- Clunkiest language?
- Biggest missed opportunity?

---

### Section 8: Recommendation

**If APPROVE:** Ready for publication as-is
**If MINOR FIXES:** List 2-3 hour fix (specific changes)
**If MAJOR REVISION:** List what needs complete rewrite
**If REJECT:** Explain why fundamentally flawed

---

## Quality Standards Reference

**From CLAUDE.md:**
- Source verification mandatory
- Natural Russian for tech audience
- Concrete examples with data
- Actionable advice (not vague platitudes)
- Evidence-based (not opinion)

**From previous feedback:**
- No robotic language
- No bullets in body
- Cut unnecessary length
- Maintain core idea (ИИ похож на человека → use existing methods → adapt them)

---

## Critical Questions to Answer

1. **Can reader apply decision framework tomorrow?** (Actionability test)
2. **Does each method have genuine success AND failure examples?** (Balance test)
3. **Is Russian natural or translated?** (Language test)
4. **Is evidence tier 1 and cited?** (Quality test)
5. **Is adaptation framing maintained?** (Message test)
6. **Is v3 better than v2 for user's goal?** (Value test)

---

**Review Mode:** ROAST (maximum brutality, find every flaw)
**Output File:** `tasks/post2-hitl-focus/review-v3.md`
**Focus:** Actionability, balance, language quality, evidence quality, value over v2
