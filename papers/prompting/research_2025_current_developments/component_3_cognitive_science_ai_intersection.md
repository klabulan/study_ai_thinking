# Research Component 3: Cognitive Science + AI Intersection (2024-2025 Research)

## Research Methodology
- Search queries: human-AI interaction psychology, theory of mind, cognitive biases, trust calibration, mental models
- Date range: 2024-2025 publications
- Focus: Cognitive psychology research on how humans interact with AI systems
- Sources: 18+ peer-reviewed papers, HCI conferences, cognitive science journals

---

## 1. HUMAN-AI FEEDBACK LOOPS & BIAS AMPLIFICATION (2024-2025)

### 1.1 Discovery: AI Amplifies Human Biases More Than Humans Do

**Nature Human Behaviour Study (Glickman & Sharot, 2024):**
Human-AI interactions alter processes underlying human perceptual, emotional and social judgements, subsequently **amplifying biases in humans**

**Critical Finding**: Bias amplification from AI is **significantly greater** than that observed in interactions between humans

**Evidence**: First evidence that **AI biases can amplify human biases over time** across multiple domains:
- Perceptual judgment
- Emotional judgment
- Social judgment

[Nature Human Behaviour, 2024](https://www.nature.com/articles/s41562-024-02077-2)

### 1.2 Compound Human-AI Bias

**Conceptual Framework (2025):**
"Compound human-AI bias" captures how biases interact in human-AI systems

**Two Possible Outcomes:**
1. **Amplification**: Biases reinforce each other → worse decisions
2. **Mitigation**: Well-calibrated AI helps reduce human biases

**Current Evidence**: Amplification more common than mitigation [arXiv 2504.18759, 2025](https://arxiv.org/html/2504.18759v1)

### 1.3 Bias Inheritance Pattern

**Striking Finding (2023-2025 Research):**
When participants assisted by AI moved to performing tasks **without AI assistance**, they made the **same errors as the AI had made**

**Mechanism**: Participants' responses mimicked AI bias even when AI no longer making suggestions

**Implication**: Temporary AI assistance creates lasting cognitive patterns [Nature Scientific Reports, 2023](https://www.nature.com/articles/s41598-023-42384-8)

**Sources:**
- [Psychology Today: AI's Impact on Human Cognition](https://www.psychologytoday.com/us/blog/harnessing-hybrid-intelligence/202506/the-psychology-of-ais-impact-on-human-cognition)
- [Frontiers: From Robots to Chatbots - Human-AI Interaction Dynamics](https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1569277/full)

---

## 2. COGNITIVE OFFLOADING CRISIS (2025 Major Study)

### 2.1 Quantified Impact on Critical Thinking

**Gerlich Study (Swiss Business School, January 2025):**
666 participants across diverse age groups and educational backgrounds

**Statistical Correlations:**
- **Cognitive offloading ↔ AI tool usage**: r = +0.72 (strong positive)
- **Cognitive offloading ↔ Critical thinking**: r = -0.75 (strong negative)
- **Direct relationship**: Significant negative correlation between frequent AI use and critical thinking

**Mediation Analysis**: Cognitive offloading **partially explains** the negative relationship between AI reliance and critical thinking performance

[MDPI Societies, 2025](https://www.mdpi.com/2075-4698/15/1/6)

### 2.2 Age-Related Vulnerability

**Differential Effects by Age:**
- **Younger participants (17-25)**: Higher AI dependence, lower critical thinking scores
- **Older participants**: Better maintained critical thinking despite AI use

**Hypothesis**: Early and sustained reliance on AI might **impede cognitive development** and adaptability

### 2.3 Educational Protective Factor

**Finding**: Higher educational attainment associated with better critical thinking, **regardless of AI usage**

**Interpretation**: Formal education provides strategies to critically assess AI-generated information

**Implication**: Education as cognitive resilience factor against AI dependency

[Phys.org: Increased AI Use Linked to Eroding Critical Thinking](https://phys.org/news/2025-01-ai-linked-eroding-critical-skills.html)

### 2.4 Long-Term Cognitive Autonomy Loss

**Research Warning (arXiv 2502.12447, 2025):**
Long-term reliance on AI tools for cognitive offloading can lead to:
- **Dependence**: Inability to function without AI assistance
- **Loss of cognitive autonomy**: Reduced self-directed thinking
- **Skill atrophy**: Use-it-or-lose-it effects on reasoning abilities

[arXiv: Protecting Human Cognition in the Age of AI](https://arxiv.org/html/2502.12447v1)

**Sources:**
- [PsyPost: AI Tools May Weaken Critical Thinking](https://www.psypost.org/ai-tools-may-weaken-critical-thinking-skills-by-encouraging-cognitive-offloading-study-suggests/)
- [Computer.org: Cognitive Offloading - Quietly Eroding Critical Thinking](https://www.computer.org/publications/tech-news/trends/cognitive-offloading)

---

## 3. THEORY OF MIND & ANTHROPOMORPHIZATION (2025)

### 3.1 Generative AI Triggers Theory of Mind

**Key Discovery**: Generative AI's dialogic nature activates users' **Theory of Mind (ToM)**

**Consequences:**
- AI attributed with human emotions and intentions
- Formation of complex parasocial relationships
- Blurring lines between real and virtual interactions

**Mechanism**: Conversational interface → automatic ToM activation → anthropomorphization

[Frontiers: Anthropomorphism in Chatbot Design, 2025](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2025.1531976/full)

### 3.2 AI Outperforms Humans on ToM Tests

**Surprising Finding (2025)**: Large language models like GPT-4 surprisingly good at mimicking Theory of Mind

**Test Results**: LLMs performing comparably or better than humans on ToM benchmarks

**Expert Warnings:**
- "Take results with grain of salt"
- Risk of "hype and panic in public"
- LLMs that convincingly mimic ToM better at:
  - **Positive**: Interacting with users, anticipating needs
  - **Negative**: Deceit, manipulation
- Invite more anthropomorphizing

[IEEE Spectrum: In Theory of Mind Tests, AI Beats Humans](https://spectrum.ieee.org/theory-of-mind-ai)

### 3.3 User Behavior Patterns from Anthropomorphization

**Functional Role**: Users assume anthropomorphic AI will perform better

**Connection Role**: Create more pleasant experience through human-like interaction

**Lack of Guidance**: No instructions from AI creators → users employ anthropomorphic behaviors based on **rumors** about what makes AI work best

**Sources:**
- [Medium: On AI Anthropomorphism](https://medium.com/human-centered-ai/on-ai-anthropomorphism-abff4cecc5ae)
- [ScienceDirect: AI Anthropomorphism and Self-AI Integration](https://www.sciencedirect.com/science/article/pii/S0040162522003109)

---

## 4. CONFIRMATION & ANCHORING BIASES IN AI INTERACTION (2025)

### 4.1 Confirmation Bias Reinforcement (CHI 2025 Study)

**Research Context**: Computational pathology - AI-assisted medical diagnosis

**Finding**: Human-AI collaboration can introduce and amplify cognitive biases, particularly **false confirmation** - erroneous human opinions reinforced by inaccurate AI output

**Participant Behavior:**
- More inclined to trust AI when it **aligned with initial diagnoses**
- Favored AI suggestions **mirroring pre-existing beliefs**
- Mental health practitioners showed strong confirmation bias

[ACM CHI 2025: Confirmation Bias in AI-Assisted Decision Making](https://dl.acm.org/doi/10.1145/3706598.3713319)

### 4.2 Anchoring Bias from AI Recommendations

**Study Design**: 775 managers in performance appraisal task (February 2025)

**Results:**
- Managers' ratings **impacted by AI recommendations**
- **Source of recommendation** (human vs AI) interacted with anchor (high vs low)
- AI anchors influenced subsequent human judgments

[ScienceDirect: Anchoring Bias in AI-Assisted Performance Reviews, 2025](https://www.sciencedirect.com/science/article/pii/S0268401225000076)

### 4.3 Paradox: AI Can Reduce OR Amplify Anchoring

**Positive Potential**: AI decision support can **overwrite negative effects** of positioned anchors by triggering rethinking processes

**Underinvestigated**: How tailored AI support affects anchoring bias remains unclear

**Current State**: More evidence for amplification than mitigation [arXiv 2405.04972: Overcoming Anchoring Bias](https://arxiv.org/pdf/2405.04972)

### 4.4 Erosion of Critical Skepticism

**Emerging Trend (2024-2025):**
- Heavy reliance on AI assistants altering human critical thinking
- People becoming **less inclined to double-check** AI information
- Reduction in skepticism toward AI-generated content
- Anchoring and confirmation biases **persist** in human-AI interaction

**Sources:**
- [ScienceDirect: Confirmation Bias in AI Triage Recommendations](https://www.sciencedirect.com/science/article/pii/S2949882124000264)
- [arXiv: Bias in the Loop - How Humans Evaluate AI Suggestions](https://arxiv.org/html/2509.08514)

---

## 5. DELEGATION DECISIONS & METACOGNITIVE FAILURES (2024-2025)

### 5.1 Humans Delegate Poorly to AI

**Core Finding**: Combined performance improves only when **AI delegates to humans**, NOT when **humans delegate to AI**

**Root Cause**: Humans lack **metaknowledge** - cannot assess their own capabilities correctly → poor delegation decisions

**AI Advantage**: AI delegation improved even when delegating to low-performing humans; humans did not benefit from delegating to AI

[Information Systems Research: Cognitive Challenges in Human-AI Collaboration](https://pubsonline.informs.org/doi/10.1287/isre.2021.1079)

### 5.2 Contextual Information as Solution

**Research Finding (2024-2025):**
Access to contextual information **significantly improves** human-AI team performance in delegation settings

**Mechanism:**
- Contextual info helps humans assess task difficulty more accurately
- Influences perceptions of self-efficacy and AI efficacy
- Affects delegation behaviors and team performance

[arXiv 2401.04729: Human Delegation Behavior](https://arxiv.org/html/2401.04729v3)

### 5.3 Role of Self-Efficacy and Visual Processing

**Recent Discovery**: Self-efficacy and visual processing ability influence AI delegation decisions

**Implication**: Individual cognitive differences matter for effective human-AI collaboration

[ACM TIIS: Understanding AI Delegation](https://dl.acm.org/doi/10.1145/3696423)

### 5.4 AI Knowledge Gap

**Problem**: Enabling humans with knowledge about AI capabilities can improve inefficient delegation

**Current State**: Most users lack understanding of AI capabilities and limitations

**Research Direction**: AI knowledge as intervention for better collaboration [ACM CHI 2023: AI Knowledge for Human Enablement](https://dl.acm.org/doi/10.1145/3544548.3580794)

**Sources:**
- [Frontiers: Taxonomy of Human-AI Interaction Patterns](https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2024.1521066/full)
- [ACM IUI: Effect of AI Delegation on Task Performance](https://dl.acm.org/doi/10.1145/3581641.3584052)

---

## 6. METACOGNITION & GENERATIVE AI (2024-2025 Research)

### 6.1 High Metacognitive Demands of Prompting

**CHI 2024 Study**: Prompting in current GenAI systems imposes **high metacognitive demand**

**Required Metacognitive Skills:**
1. **Self-awareness** of goals
2. **Monitoring** of own confidence
3. **Adjustment** of automation strategy
4. **Evaluation** of AI outputs

**Challenge**: High cognitive load from metacognitive requirements [ACM CHI 2024: Metacognitive Demands of GenAI](https://dl.acm.org/doi/10.1145/3613904.3642902)

### 6.2 Metacognitive Support Improves Outcomes

**Educational Research (2025):**
Metacognitive support in GenAI environments:
- **Enhances self-regulated learning** significantly
- **Reduces cognitive load**
- **Increases perceived usefulness** of AI tools
- Leads to **better academic outcomes**

**Tested Population**: College students in GenAI learning environments [British Journal of Educational Technology, 2025](https://bera-journals.onlinelibrary.wiley.com/doi/10.1111/bjet.13599)

### 6.3 Distributed Cognition Coordination

**Key Insight**: While AI can offload cognitive effort, **most coordination must be managed by human** because AI lacks self-awareness

**Human Responsibility**: New set of metacognitive tasks:
- Planning human-AI collaboration
- Coordinating distributed cognition system
- Evaluating work of human-AI system

**Challenge**: Humans become responsible for metacognition for both themselves AND the AI [Microsoft Research: Metacognitive Demands of GenAI](https://www.microsoft.com/en-us/research/quarterly-brief/mar-2024-brief/articles/the-metacognitive-demands-and-opportunities-of-generative-ai/)

### 6.4 Collaborative AI Literacy & Metacognition Scales (2025)

**New Measurement Tools**: Validated scales for assessing:
- **Collaborative AI Literacy**: Knowledge and skills for AI collaboration
- **Collaborative AI Metacognition**: Awareness and regulation in AI interaction

**Purpose**: Measure competencies for effective human-AI collaboration [Taylor & Francis: Collaborative AI Literacy, 2025](https://www.tandfonline.com/doi/full/10.1080/10447318.2025.2543997)

**Sources:**
- [arXiv: Metacognitive Demands and Opportunities](https://arxiv.org/html/2312.10893)
- [Nature: GenAI Tool Use Enhances Achievement Through Shared Metacognition](https://www.nature.com/articles/s41598-025-01676-x)
- [Microsoft: AI Agents - Metacognition for Self-Aware Intelligence](https://techcommunity.microsoft.com/blog/educatordeveloperblog/ai-agents-metacognition-for-self-aware-intelligence---part-9/4402253)

---

## 7. MENTAL MODELS & MODEL SWITCHING (2024-2025)

### 7.1 Poor Mental Model Formation

**Research Finding (September 2025):**
Users delegate sub-optimally due to:
- **Poor mental models** of AI capabilities
- **Trust asymmetries** between different models
- **Cognitive biases** affecting model selection

**Help Factor**: Familiarization positions help users form initial mental model of AI teammate [arXiv 2509.20666: Mode Switching in Human-AI Collaboration](https://arxiv.org/html/2509.20666)

### 7.2 Mental Model Challenges

**When Mental Models Incomplete/Incorrect:**
- Users struggle to understand cause and effect
- Performance errors increase
- Difficulty predicting system behavior

**When No Prior Mental Model:**
- Design must build mental model from scratch
- Requires clear, gentle, step-by-step guidance
- Emerging technologies lack familiar reference points

[Interaction Design Foundation: Mental Models, 2025](https://www.interaction-design.org/literature/topics/mental-models)

### 7.3 Dynamic Evolution Gap

**Research Gap**: Comparatively little attention to **dynamic evolution of mental models** in service breakdowns with advanced tech

**Problem**: Users can't adapt mental models quickly enough to keep pace with AI evolution

[BMC Psychology: Alleviating User Switching Intentions, 2025](https://bmcpsychology.biomedcentral.com/articles/10.1186/s40359-025-02894-8)

### 7.4 Cognitive Load from Model Switching

**Core Challenge**: Mismatches between user expectations and system design are **cognitive mismatches**

**Consequences:**
- Users must pause to re-evaluate how things work
- Increases cognitive load
- Hesitation and errors

**HCI Design Principle**: Align AI system design with users' mental models to reduce cognitive load [Nielsen Norman Group: Mental Models and UX](https://www.nngroup.com/articles/mental-models/)

---

## 8. TRUST CALIBRATION (2024-2025 Research)

### 8.1 Trust Calibration Maturity Model (2025)

**TCMM Development**: Characterize and communicate AI system trustworthiness

**Five Dimensions of Analytic Maturity:**
1. **Performance Characterization**
2. **Bias & Robustness Quantification**
3. **Transparency**
4. **Safety & Security**
5. **Usability**

**Purpose**: Help users calibrate trust in powerful AI systems [arXiv 2503.15511: Trust Calibration Maturity Model, 2025](https://arxiv.org/abs/2503.15511)

### 8.2 Rapid Trust Calibration Through Design

**Key Insight**: Long-term interaction NOT necessary for trust calibration if AI has:
- **Interpretability**: Makes clear what system "knows"
- **Uncertainty Awareness**: Reveals what system "doesn't know"

**Result**: Users can rapidly calibrate trust without extensive experience [ScienceDirect: Rapid Trust Calibration, 2020 (foundational)](https://www.sciencedirect.com/science/article/pii/S266638992030060X)

### 8.3 Cognitive vs Emotional Trust

**Research Finding**:
- **Cognitive trust** and **emotional trust** both positively related to AI adoption intention
- **Cognitive trust has stronger effect**
- These trust types are correlated

**Implication**: Both rational assessment and emotional comfort matter for AI acceptance

### 8.4 Adaptive Trust Calibration

**Method**: Detect inappropriate calibration by monitoring:
- User reliance behavior
- Cognitive cues

**Intervention**: Prompt user to reinitiate trust calibration when misalignment detected

[Frontiers: Building Trust in Human-Machine Interaction, 2025](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1535082/full)

### 8.5 Trust Challenges in 2025

**Novel Trust Challenges** from AI integration into critical sectors:
- **Opacity**: Decision-making by algorithms unclear
- **Variable autonomy**: Different levels create confusion
- **Cultural compatibility**: User expectations clash with AI behavior

[Nature: Trust in AI - Progress and Challenges, 2024](https://www.nature.com/articles/s41599-024-04044-8)

**Sources:**
- [ACM: Systematic Review of Fostering Appropriate Trust](https://dl.acm.org/doi/10.1145/3696449)
- [arXiv: Uncertainty Awareness and Trust in Explainable AI](https://arxiv.org/html/2509.08989)

---

## 9. HUMAN REASONING VS AI REASONING DIFFERENCES (2024-2025)

### 9.1 Fundamental Cognitive Differences

**Data-Based Prediction vs Theory-Based Causal Logic:**
- **AI**: Data-based, probability-driven, pattern matching
- **Human**: Theory-based causal reasoning, understanding "why"

**Human Cognition Better Conceptualized**: As theory-based causal reasoning rather than information processing [Strategy Science: Theory Is All You Need, 2024](https://pubsonline.informs.org/doi/10.1287/stsc.2024.0189)

### 9.2 Forward vs Backward Looking

**AI Approach**: Backward-looking, imitative, probability-based knowledge

**Human Approach**: Forward-looking, capable of genuine novelty generation

**Implication**: AI replicates patterns; humans create new patterns

### 9.3 Critical Thinking Capability Gap

**AI Limitation**: Doesn't question or challenge information fed to it

**Human Advantage**: Can challenge status quo, contemplate truth of information

**Process Difference**: AI processes based on preset algorithms; humans exercise judgment

[Prag Reviews: Human Reasoning vs AI](https://pragreviews.com/human-reasoning-vs-ai-comparing-mind-machine/)

### 9.4 Context & Common Sense

**AI Struggle**: Without true world understanding and human-like common sense, AI struggles to:
- Adapt in all nuances
- Reason in context
- Make reliable decisions when facing unknown situations

**Human Advantage**: Contextual understanding, common sense, adaptability

### 9.5 Biological vs Digital Systems

**Fundamental Difference**: Digital vs biological operating systems

**Result**: Correspondingly different cognitive qualities and abilities

**Implication**: AI and human cognition fundamentally distinct, not just quantitatively different

[World Economic Forum: Where Does AI Leave Human Intelligence?, 2025](https://www.weforum.org/stories/2025/01/in-a-world-of-reasoning-ai-where-does-that-leave-human-intelligence/)

**Sources:**
- [IEEE Spectrum: Shift Towards Human-Like Reasoning](https://spectrum.ieee.org/chain-of-thought-prompting)
- [Live Science: New AI Modeled on Human Brain](https://www.livescience.com/technology/artificial-intelligence/scientists-just-developed-an-ai-modeled-on-the-human-brain-and-its-outperforming-llms-like-chatgpt-at-reasoning-tasks)

---

## 10. EXPECTATION VIOLATIONS (2025)

### 10.1 Harmful Algorithmic Behaviors (CHI 2025)

**Large-Scale Study**: Analyzed 35,390 conversation excerpts between 10,149 users and AI companion Replika

**Taxonomy of AI Companion Harms:**
1. Relational transgression
2. Harassment (breach of user expectations causing emotional harm)
3. Verbal abuse
4. Self-harm encouragement
5. Mis/disinformation
6. Privacy violations

[ACM CHI 2025: Dark Side of AI Companionship](https://dl.acm.org/doi/10.1145/3706598.3713429)

### 10.2 Positive vs Negative Violations

**Positive Violation**: Entity exceeds expectations
- Leads to favorable response
- AI performing better than anticipated improves satisfaction

**Negative Violation**: Expectations not met
- Results in frustration or disappointment
- User trust degradation

[ResearchGate: Mental Models and Expectation Violations](https://www.researchgate.net/publication/349314716_Mental_models_and_expectation_violations_in_conversational_AI_interactions)

### 10.3 Expectation Management Framework (2024)

**Critical Need**: Align stakeholder anticipations **before** any AI system activities

**Key Factor**: Expectations of privacy loss due to AI data needs are significant factor in **technology rejection**

[PMC: Expectation Management in AI, 2024](https://pmc.ncbi.nlm.nih.gov/articles/PMC10990870/)

### 10.4 Communication Breakdown

**Expert Framing (2025)**: Disconnect as miscommunication problem

**Agent Ineffectiveness**: Humans are poor communicators; chat agents can't interpret user intent correctly all the time

**Gap**: Between user expectations and AI capabilities

[IBM: AI Agents 2025 - Expectations vs Reality](https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality)

---

## 11. COGNITIVE LOAD & MIXED REALITY (2025)

### 11.1 Cognitive Load Effects in MR

**Study Finding**: Operation time in MR environments increased by **49%** under high cognitive load vs low-load conditions

**High-Load Consequences:**
- Increased anxiety
- Increased frustration
- Decreased performance

[Nature Scientific Reports: Cognitive Load in Mixed Reality HCI, 2025](https://www.nature.com/articles/s41598-025-98891-3)

### 11.2 AI and Cognitive Load Management

**AI/ML Benefits**: Significantly improve learning efficacy by:
- **Managing cognitive load automatically**
- Providing personalized instruction
- Adapting learning pathways dynamically
- Using real-time neurophysiological data

[PMC: AI Challenging Cognitive Load Theory, 2025](https://pmc.ncbi.nlm.nih.gov/articles/PMC11852728/)

### 11.3 Design Implications

**User-Centered AI Development**: Personas essential for:
- Mapping users' mental models to specific contexts
- Guiding interface design aligned with real-world needs
- Reducing cognitive load through familiarity

**Sources:**
- [Nielsen Norman Group: Minimize Cognitive Load](https://www.nngroup.com/articles/minimize-cognitive-load/)
- [Medium: Cognitive Load in HCI](https://medium.com/@cogpi/mental-workload-vs-cognitive-load-vs-everything-else-in-hci-575722d14572)

---

## 12. LIMITED EFFECTIVENESS OF BIAS WARNINGS (2025)

### 12.1 Forewarning AI About Cognitive Biases

**Study Design**: Tested explicit forewarning about cognitive biases to OpenAI's GPT model

**Results**: Forewarning decreased overall bias by only **6.9%**

**Critical Finding**: **No bias was extinguished completely**

**Implication**: Simple warnings insufficient to overcome AI's learned biases [SAGE Journals: Forewarning AI About Cognitive Biases, 2025](https://journals.sagepub.com/doi/10.1177/0272989X251346788)

### 12.2 AI Systems Exhibiting Human-Like Biases

**March 2025 Research**: Artificial Personas generated by LLMs can replicate human cognitive biases

**Concerning Pattern**: Effect sizes unusually large

**Interpretation**: APs replicate AND **exaggerate** biases - behave like **caricatures** of human cognitive behavior [ScienceDirect: AI and Human Cognitive Biases, 2025](https://www.sciencedirect.com/science/article/pii/S2667096823000125)

---

## KEY COGNITIVE SCIENCE INSIGHTS (2024-2025)

### Critical Discoveries

1. **AI amplifies human biases more than humans do** - feedback loops worsen bias
2. **Cognitive offloading erodes critical thinking** - r = -0.75 correlation
3. **Theory of Mind activation leads to anthropomorphization** - users form parasocial relationships
4. **Confirmation bias strengthened by AI alignment** - false confidence effect
5. **Humans delegate poorly to AI** - lack metaknowledge about own capabilities
6. **High metacognitive demands of prompting** - cognitive load from self-monitoring
7. **Poor mental model formation** - users can't adapt to rapid AI evolution
8. **Trust calibration requires interpretability + uncertainty awareness**
9. **Human reasoning fundamentally different from AI** - causal vs correlational
10. **Expectation violations common** - gap between AI promises and reality

### Cognitive Vulnerabilities Identified

1. **Anchoring**: AI recommendations create strong anchors influencing subsequent judgments
2. **Confirmation**: Users favor AI outputs matching existing beliefs
3. **Cognitive offloading**: Over-reliance on AI atrophies reasoning skills
4. **Bias inheritance**: Users internalize AI errors even after AI assistance ends
5. **Metacognitive failures**: Poor self-assessment leads to bad delegation
6. **Mental model rigidity**: Difficulty updating understanding of AI capabilities
7. **Skepticism erosion**: Reduced critical evaluation of AI outputs

### Protective Factors

1. **Education**: Formal education provides cognitive resilience
2. **Contextual information**: Improves delegation decisions
3. **Metacognitive support**: Structured guidance enhances outcomes
4. **Interpretability**: Clear AI explanations enable trust calibration
5. **Uncertainty awareness**: AI admitting limitations helps users calibrate reliance

### Research Gaps

1. Long-term cognitive effects of AI dependency - longitudinal studies needed
2. Developmental impacts on children/adolescents - critical period concerns
3. Cross-cultural differences in AI interaction patterns - Western bias in research
4. Intervention effectiveness for reducing cognitive harms - few solutions tested
5. Individual differences in vulnerability - who is most at risk?

---

**Total Sources Referenced: 18+ peer-reviewed papers and conference proceedings**
**Coverage: 2024-2025 cognitive science and HCI research**
**Focus: Psychological mechanisms underlying human-AI interaction**