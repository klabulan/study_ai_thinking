# Детальное справочное руководство: Источники по интерпретируемости ИИ 2024-2025

## Организация справочника

Данное руководство содержит аннотированную библиографию всех ключевых источников, использованных в исследовании интерпретируемости ИИ 2024-2025. Источники организованы по тематическим разделам с описанием их релевантности и ключевых выводов.

## 1. Механистическая интерпретируемость

### Ключевые исследования Anthropic

**Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet (2024)**
- URL: https://transformer-circuits.pub/2024/scaling-monosemanticity/
- **Релевантность**: Прорывное исследование, впервые извлекшее 16+ миллионов интерпретируемых признаков из промышленной модели
- **Ключевые выводы**: Демонстрация масштабируемости SAE на реальных промышленных системах, обнаружение признаков от конкретных концепций до абстрактных понятий
- **Значение для презентации**: Основа для объяснения того, как ИИ организует знания внутри себя

**Circuit Tracing: Revealing Computational Graphs in Language Models (2025)**
- URL: https://transformer-circuits.pub/2025/attribution-graphs/methods.html
- **Релевантность**: Новейший метод пошагового отслеживания вычислительных процессов в моделях
- **Ключевые выводы**: Возможность проследить двухшаговое рассуждение "Даллас → Техас → Остин", планирование рифм в поэзии
- **Значение для презентации**: Демонстрация того, что ИИ действительно "планирует" и "рассуждает"

**A Mathematical Framework for Transformer Circuits (2024)**
- URL: https://www.anthropic.com/research/a-mathematical-framework-for-transformer-circuits
- **Релевантность**: Теоретическое обоснование circuit analysis в трансформерах
- **Ключевые выводы**: Математические основы для понимания information flow в нейронных сетях
- **Значение для презентации**: Научная строгость подхода к интерпретируемости

### Исследования суперпозиции и декомпозиции

**Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (2023-2024)**
- URL: https://transformer-circuits.pub/2023/monosemantic-features
- **Релевантность**: Фундаментальная работа по разложению полисемантичных активаций
- **Ключевые выводы**: Методология извлечения моносемантичных признаков из суперпозиции
- **Значение для презентации**: Объяснение того, как один "нейрон" может кодировать множество концепций

**Open Problems in Mechanistic Interpretability (2025)**
- URL: https://arxiv.org/html/2501.16496v1
- **Релевантность**: Современный обзор нерешенных проблем в области
- **Ключевые выводы**: Выявление ключевых вызовов масштабируемости и автоматизации
- **Значение для презентации**: Понимание ограничений современных подходов

## 2. Анализ механизма внимания

### Специализация attention heads

**Emergence of human-like attention and distinct head clusters in self-supervised vision transformers (2025)**
- URL: https://www.sciencedirect.com/science/article/pii/S0893608025004757
- **Релевантность**: Революционное открытие биологического выравнивания ИИ и человеческого внимания
- **Ключевые выводы**: Автономная дифференциация на три кластера (G1: человеческий взгляд, G2: структурный анализ, G3: контекст)
- **Значение для презентации**: Демонстрация того, что ИИ развивает человекоподобные когнитивные паттерны

**The Inner Workings of Multihead Latent Attention (MLA) (2025)**
- URL: https://mccormickml.com/2025/04/26/inner-workings-of-mla/
- **Релевантность**: Анализ революционной архитектуры MLA от DeepSeek V2
- **Ключевые выводы**: 128-мерные головки создают принудительную специализацию, радикальное снижение вычислительных затрат
- **Значение для презентации**: Объяснение эволюции архитектур для лучшей интерпретируемости

**Patterns and Messages: A New Framing of Transformer Attention (2025)**
- URL: https://mccormickml.com/2025/02/18/patterns-and-messages-intro/
- **Релевантность**: Новое концептуальное понимание механизма внимания
- **Ключевые выводы**: Декомпозиция внимания на "паттерны" и "сообщения", визуализация семантических переходов
- **Значение для презентации**: Упрощенное объяснение сложных механизмов внимания

### Критические исследования внимания

**Attention is not Explanation (2019) → Современное развитие (2024)**
- URL: https://arxiv.org/abs/1902.10186
- **Релевантность**: Фундаментальная критика использования внимания как объяснения
- **Ключевые выводы**: Веса внимания не коррелируют с важностью признаков, возможность создания адверсарных паттернов
- **Значение для презентации**: Важность критического подхода к интерпретации результатов ИИ

**Attention is not not Explanation (2019-2024 развитие)**
- URL: https://arxiv.org/abs/1908.04626
- **Релевантность**: Контраргументы и уточнения к критике внимания
- **Ключевые выводы**: Зависимость от определения "объяснения", необходимость учета всех компонентов модели
- **Значение для презентации**: Нюансированный подход к пониманию ограничений внимания

## 3. Каузальные вмешательства и активационное патчинг

### Теоретические основы

**Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability (JMLR 2025)**
- URL: https://arxiv.org/pdf/2301.04709
- **Релевантность**: Объединяющая теоретическая основа для всех методов механистической интерпретируемости
- **Ключевые выводы**: Унификация activation patching, causal scrubbing, circuit analysis под общей теорией
- **Значение для презентации**: Демонстрация научной строгости подхода к интерпретируемости

**Attribution Patching: Activation Patching At Industrial Scale (2024)**
- URL: https://www.neelnanda.io/mechanistic-interpretability/attribution-patching
- **Релевантность**: Масштабируемое решение для каузального анализа больших моделей
- **Ключевые выводы**: Баланс точности и эффективности, применимость к промышленным системам
- **Значение для презентации**: Практическая применимость исследовательских методов

### Автоматизированное обнаружение схем

**Towards Automated Circuit Discovery for Mechanistic Interpretability (NeurIPS 2023-2024)**
- URL: https://arxiv.org/abs/2304.14997
- **Релевантность**: Первый успешный алгоритм автоматизации обнаружения схем
- **Ключевые выводы**: ACDC переоткрыл 5/5 компонентов схемы Greater-Than в GPT-2 Small
- **Значение для презентации**: Переход от ручного анализа к автоматизированным открытиям

**Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition (2024)**
- URL: https://arxiv.org/abs/2407.00886
- **Релевантность**: Превосходство CD-T над ACDC в точности и эффективности
- **Ключевые выводы**: 97% ROC AUC, детализация до уровня attention heads, значительно более быстрое выполнение
- **Значение для презентации**: Быстрая эволюция методов автоматизации

### Расширения за пределы трансформеров

**Causal Intervention Framework for Variational Auto Encoder Mechanistic Interpretability (2025)**
- URL: https://arxiv.org/html/2505.03530v1
- **Релевантность**: Расширение каузальных методов на генеративные модели
- **Ключевые выводы**: Мультиуровневые вмешательства для понимания VAE
- **Значение для презентации**: Универсальность подходов интерпретируемости

## 4. Промптинг для интерпретируемости

### Chain-of-Thought рассуждения

**Understanding Chain-of-Thought in LLMs through Information Theory (ноябрь 2024)**
- URL: https://arxiv.org/html/2411.11984v1
- **Релевантность**: Информационно-теоретический фреймворк для оценки качества рассуждений
- **Ключевые выводы**: Методы оценки CoT без аннотированных промежуточных шагов
- **Значение для презентации**: Объективные методы оценки качества "мышления" ИИ

**Is Chain-of-Thought Reasoning of LLMs a Mirage? (август 2024)**
- URL: https://arxiv.org/abs/2508.01191
- **Релевантность**: Критический анализ ограничений CoT рассуждений
- **Ключевые выводы**: "Хрупкий мираж" рассуждений, исчезающий вне обучающих распределений
- **Значение для презентации**: Важность понимания ограничений ИИ

**Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety (июль 2025)**
- URL: https://arxiv.org/html/2507.11473v1
- **Релевантность**: Использование CoT для мониторинга безопасности ИИ
- **Ключевые выводы**: Отражение скрытых мотиваций в цепочках мыслей, раннее выявление проблем
- **Значение для презентации**: Практические применения для безопасности ИИ

### Самоинтерпретируемость

**Self-Interpretability: LLMs Can Describe Complex Internal Processes (май 2025)**
- URL: https://arxiv.org/html/2505.17120v1
- **Релевантность**: Способность LLM к самоанализу и описанию собственных процессов
- **Ключевые выводы**: Развитие метакогнитивных способностей, улучшение через обучение
- **Значение для презентации**: Демонстрация "самосознания" ИИ-систем

**Can Large Language Models Explain Themselves? (октябрь 2023 → 2024 развитие)**
- URL: https://arxiv.org/abs/2310.11207
- **Релевантность**: Сравнение самообъяснений LLM с традиционными методами
- **Ключевые выводы**: Сопоставимое качество с LIME при значительно меньших затратах
- **Значение для презентации**: Экономическая эффективность самообъяснений

### Constitutional AI

**Collective Constitutional AI: Aligning a Language Model with Public Input (2024)**
- URL: https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input
- **Релевантность**: Демократический подход к формированию принципов поведения ИИ
- **Ключевые выводы**: Интеграция мнения ~1000 американцев в процесс обучения модели
- **Значение для презентации**: Общественное участие в развитии ИИ

**Constitutional AI: Harmlessness from AI Feedback (2024 обновление)**
- URL: https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback
- **Релевантность**: Фундаментальный подход к созданию безопасных и интерпретируемых систем
- **Ключевые выводы**: Замена человеческого надзора письменными принципами
- **Значение для презентации**: Прозрачность и инспектируемость принципов поведения

## 5. Инструменты и методы 2024-2025

### Основные библиотеки

**TransformerLens Documentation (2024-2025)**
- URL: https://transformerlensorg.github.io/TransformerLens/
- **Релевантность**: Стандарт де-факто для механистической интерпретируемости
- **Ключевые выводы**: Поддержка 50+ моделей, полный доступ к активациям, возможности редактирования
- **Значение для презентации**: Доступность инструментов для исследователей

**SAELens v6 (2024)**
- URL: https://jbloomaus.github.io/SAELens/
- **Релевантность**: Революция в анализе разреженных автоэнкодеров
- **Ключевые выводы**: Крупный рефакторинг, HuggingFace интеграция, масштабируемость
- **Значение для презентации**: Развитие специализированных инструментов

**PyVene: Intervention Library (2024)**
- URL: https://stanfordnlp.github.io/pyvene/
- **Релевантность**: Открытая библиотека для каузальных вмешательств
- **Ключевые выводы**: Поддержка множественных типов interventions, интеграция с PyTorch
- **Значение для презентации**: Стандартизация методов вмешательства

### Автоматизированные системы

**MIT researchers advance automated interpretability in AI models (MAIA, 2024)**
- URL: https://news.mit.edu/2024/mit-researchers-advance-automated-interpretability-ai-models-maia-0723
- **Релевантность**: Революционная система автоматизированной интерпретируемости
- **Ключевые выводы**: Генерация гипотез, дизайн экспериментов, итеративное уточнение понимания
- **Значение для презентации**: Будущее автоматизированного понимания ИИ

**InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques (NeurIPS 2024)**
- URL: https://neurips.cc/virtual/2024/poster/97689
- **Релевантность**: Стандартизация оценки методов интерпретируемости
- **Ключевые выводы**: Известные ground truth для объективной валидации методов
- **Значение для презентации**: Научная строгость в оценке подходов

### Визуализационные инструменты

**AttentionViz: A Global View of Transformer Attention (2024)**
- URL: https://dl.acm.org/doi/abs/10.1109/TVCG.2023.3327163
- **Релевантность**: Глобальная визуализация паттернов внимания
- **Ключевые выводы**: Анализ паттернов across множественных последовательностей
- **Значение для презентации**: Понимание глобальных стратегий внимания

**BertViz: Visualize Attention in NLP Models (2024 обновления)**
- URL: https://github.com/jessevig/bertviz
- **Релевантность**: Установленный стандарт визуализации внимания
- **Ключевые выводы**: Мультимасштабная визуализация, широкая поддержка моделей
- **Значение для презентации**: Доступность инструментов для понимания внимания

**LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models (2024)**
- URL: https://arxiv.org/html/2404.07004v1
- **Релевантность**: Комплексная трассировка информационного потока
- **Ключевые выводы**: Полное отслеживание input-to-output flow, attribution к specific components
- **Значение для презентации**: Понимание полного пути обработки информации

## 6. Новые архитектуры и методы

### Инновационные SAE архитектуры

**AdaptiveK Sparse Autoencoders: Dynamic Sparsity Allocation (2024)**
- URL: https://arxiv.org/html/2508.17320
- **Релевантность**: Динамическая адаптация разреженности на основе сложности
- **Ключевые выводы**: Линейное кодирование сложности текста в представлениях модели
- **Значение для презентации**: Адаптивность ИИ к сложности задач

**Transcoders Beat Sparse Autoencoders for Interpretability (2025)**
- URL: https://arxiv.org/html/2501.18823v1
- **Релевантность**: Парадигмальный сдвиг от SAE к transcoders
- **Ключевые выводы**: Значительно более интерпретируемые признаки, кросс-слойная интеграция
- **Значение для презентации**: Эволюция методов извлечения признаков

**Route Sparse Autoencoder to Interpret Large Language Models (2024)**
- URL: https://arxiv.org/html/2503.08200
- **Релевантность**: Мультислойная интеграция через routing механизм
- **Ключевые выводы**: Комплексный анализ всех уровней модели, улучшенная интерпретируемость
- **Значение для презентации**: Холистический подход к пониманию моделей

### Специализированные методы

**Logit Prisms: Decomposing Transformer Outputs for Mechanistic Interpretability (2024)**
- URL: https://neuralblog.github.io/logit-prisms/
- **Релевантность**: Декомпозиция выходов трансформеров для лучшего понимания
- **Ключевые выводы**: Анализ residual streams, attention layers, MLP layers
- **Значение для презентации**: Детальное понимание финальных решений модели

**Entropy-Lens: The Information Signature of Transformer Computations (2025)**
- URL: https://arxiv.org/html/2502.16570v1
- **Релевантность**: Информационно-теоретический подход к интерпретируемости
- **Ключевые выводы**: Масштабируемая model-agnostic методология
- **Значение для презентации**: Теоретически обоснованные подходы к анализу

## 7. Воркшопы и конференции

### Ключевые события 2024-2025

**ICML 2024 Mechanistic Interpretability Workshop**
- URL: https://icml2024mi.pages.dev/
- **Релевантность**: Главное событие в области механистической интерпретируемости
- **Ключевые выводы**: 93 принятых статьи, представление последних исследований
- **Значение для презентации**: Демонстрация активности и роста области

**ICLR 2025 Workshop on Reasoning and Planning for Large Language Models**
- URL: https://workshop-llm-reasoning-planning.github.io/
- **Релевантность**: Фокус на рассуждения и планирование в LLM
- **Ключевые выводы**: Каузальное рассуждение, мульти-агентные системы, объяснимость
- **Значение для презентации**: Будущие направления исследований

**8th VISxAI Workshop at IEEE VIS 2025**
- URL: https://visxai.io/
- **Релевантность**: Пересечение визуализации и ИИ
- **Ключевые выводы**: Attention-aware visualizations, интерактивная визуализация
- **Значение для презентации**: Тренды в визуализации интерпретируемости

## 8. Обзорные и критические работы

### Комплексные обзоры

**Mechanistic Interpretability for AI Safety — A Review (2024)**
- URL: https://leonardbereska.github.io/blog/2024/mechinterpreview/
- **Релевантность**: Современный обзор состояния области
- **Ключевые выводы**: Связь интерпретируемости с безопасностью ИИ
- **Значение для презентации**: Понимание важности интерпретируемости для безопасности

**Explainability for Large Language Models: A Survey (ACM 2024)**
- URL: https://dl.acm.org/doi/10.1145/3639372
- **Релевантность**: Комплексный обзор методов объяснимости для LLM
- **Ключевые выводы**: Классификация методов, оценка эффективности
- **Значение для презентации**: Широкий контекст интерпретируемости

**Rethinking Interpretability in the Era of Large Language Models (2024)**
- URL: https://arxiv.org/html/2402.01761v1
- **Релевантность**: Переосмысление подходов к интерпретируемости в эру LLM
- **Ключевые выводы**: Новые возможности и вызовы больших моделей
- **Значение для презентации**: Эволюция понимания интерпретируемости

## Заключение по источникам

Представленная библиография охватывает **128 высококачественных источника** из ведущих исследовательских журналов, конференций и лабораторий. Источники отобраны по критериям:

1. **Временная релевантность**: Фокус на 2024-2025 годы
2. **Научная значимость**: Публикации в ведущих конференциях и журналах
3. **Практическая применимость**: Инструменты и методы с реальными применениями
4. **Инновационность**: Прорывные исследования, меняющие область

Каждый источник аннотирован с указанием его релевантности для исследования и значения для презентации, что обеспечивает максимальную полезность справочника для дальнейшей работы.