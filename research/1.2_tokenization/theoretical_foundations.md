# Теоретические основы токенизации - Промежуточные результаты

## Источники исследования

### Основные научные работы
1. Sennrich, Rico, et al. "Neural Machine Translation of Rare Words with Subword Units" (2016) - основополагающая работа по BPE
2. "An Analysis of BPE Vocabulary Trimming in Neural Machine Translation" (2024) - ACL Anthology
3. "SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing" (2018)
4. "Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP" (2021)
5. "Entropy-Driven Pre-Tokenization for Byte-Pair Encoding" (2025)

### Дополнительные материалы
6. Исследования влияния размера словаря на производительность моделей (2024)
7. Анализ токенизации для украинского языка (2025)
8. Проблемы предвзятости токенизаторов в больших языковых моделях (2024)
9. Сравнительный анализ методов subword токенизации
10. Эволюция методов токенизации в НЛП

## Ключевые выводы

### 1. Принципы преобразования текста в числа

**Базовый процесс:**
- Токенизация: разделение текста на минимальные единицы (токены)
- Кодирование: преобразование токенов в числовые идентификаторы
- Эмбеддинг: преобразование идентификаторов в векторные представления
- Контекстуализация: учет взаимосвязей между токенами через механизмы внимания

**Аналогия для презентации:** Как человек читает незнакомое слово - сначала разбивает на знакомые части (слоги), затем понимает значение каждой части, и наконец объединяет в общий смысл с учетом контекста.

### 2. Эволюция методов токенизации

**Хронология развития:**
- **Word-level** (до 2010-х): разделение по словам, простое но ограниченное
- **Character-level** (2010-2015): посимвольная обработка, гибкое но неэффективное
- **Subword-level** (2016-настоящее время): оптимальный баланс между эффективностью и гибкостью

**Проблемы решаемые эволюцией:**
- Out-of-vocabulary (OOV) слова
- Размер словаря vs качество представления
- Обработка редких слов и морфологически богатых языков
- Баланс между скоростью и точностью

### 3. Сравнение BPE и SentencePiece

**Byte Pair Encoding (BPE):**
- Основа: статистический анализ частотности пар символов
- Преимущества: простота, хорошая компрессия, широкое применение
- Ограничения: требует предварительной токенизации, языково-зависимый

**SentencePiece:**
- Основа: объединение BPE и Unigram Language Model
- Преимущества: языково-независимый, работает с raw text, гибкость
- Применение: многоязычные модели, модели с фиксированным размером словаря

**Практическое применение в 2024:**
- GPT-4: BPE с словарем 100,256 токенов
- GPT-4o: увеличенный словарь до 199,997 токенов для лучшей эффективности
- T5, mT5: SentencePiece для многоязычности

### 4. Влияние размера словаря на производительность

**Современные тенденции (2024):**
- Увеличение размеров словарей в 2 раза по сравнению с предыдущими моделями
- GPT-4o: 10-кратное увеличение кириллических токенов (с 435 до 4,660)
- Улучшение скорости инференса за счет лучшей компрессии

**Компромиссы:**
- Больший словарь = лучшая компрессия, но больше параметров
- Оптимальный размер зависит от языка и задачи
- Необходимость баланса между покрытием лексики и вычислительной эффективностью

## Аналогии для презентации

### 1. Токенизация как разбор незнакомого слова
"Представьте, что вы читаете слово 'подосиновики'. Не зная его значения, вы интуитивно разбиваете его на знакомые части: 'под-осин-ов-ик-и'. Каждая часть несет смысл, и вместе они дают полное понимание."

### 2. Эволюция как развитие навыков чтения
"Сначала дети читают по буквам (character-level), затем по слогам (subword), и наконец целыми словами (word-level). ИИ прошел обратный путь - от попыток читать целыми словами к оптимальному subword подходу."

### 3. Размер словаря как личный лексикон
"Чем больше ваш словарный запас, тем точнее вы выражаете мысли. Но есть предел - слишком специфические термины замедляют общение. ИИ сталкивается с той же дилеммой."

## Следующие шаги исследования

1. Изучение практических аспектов влияния токенизации на качество
2. Анализ особенностей для русского языка
3. Современные подходы и инновации 2023-2024
4. Практические рекомендации для пользователей